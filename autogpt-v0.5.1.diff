diff --git a/autogpts/autogpt/autogpt/app/utils.py b/autogpts/autogpt/autogpt/app/utils.py
index 49a1e2a4..b4724bc4 100644
--- a/autogpts/autogpt/autogpt/app/utils.py
+++ b/autogpts/autogpt/autogpt/app/utils.py
@@ -74,6 +74,7 @@ def get_bulletin_from_web():
 
 
 def get_current_git_branch() -> str:
+    return "autogpt-v0.5.1"
     try:
         repo = Repo(search_parent_directories=True)
         branch = repo.active_branch
diff --git a/autogpts/forge/forge/agent.py b/autogpts/forge/forge/agent.py
index 7757c668..c408282d 100644
--- a/autogpts/forge/forge/agent.py
+++ b/autogpts/forge/forge/agent.py
@@ -1,5 +1,7 @@
+import json
 from forge.actions import ActionRegister
 from forge.sdk import (
+    chat_completion_request,
     Agent,
     AgentDB,
     ForgeLogger,
@@ -107,7 +109,6 @@ class ForgeAgent(Agent):
         If you want to get the task use:
 
         ```
-        task = await self.db.get_task(task_id)
         ```
 
         The step request body is essentially the same as the task request and contains an input
@@ -120,6 +121,8 @@ class ForgeAgent(Agent):
         if they want the agent to continue or not.
         """
         # An example that
+        task = await self.db.get_task(task_id)
+
         step = await self.db.create_step(
             task_id=task_id, input=step_request, is_last=True
         )
@@ -134,13 +137,42 @@ class ForgeAgent(Agent):
             agent_created=True,
         )
 
-        step.output = "Washington D.C"
+        step.output = "WashingtC"
 
         LOG.info(
             f"\tâœ… Final Step completed: {step.step_id}. \n"
-            + f"Output should be placeholder text Washington D.C. You'll need to \n"
+            + f"Output shoulington D.C. You'll need to \n"
             + f"modify execute_step to include LLM behavior. Follow the tutorial "
             + f"if confused. "
         )
-
+        from .sdk import PromptEngine
+
+        prompt_engine = PromptEngine("gpt-3.5-turbo")
+        system_prompt = prompt_engine.load_prompt("system-format")
+
+        task_kwargs = {
+            "task": task.input,
+            "abilities": self.abilities.list_abilities_for_prompt(),
+        }
+        task_prompt = prompt_engine.load_prompt("task-step", **task_kwargs)
+        messages = [
+            {"role": "system", "content": system_prompt},
+            {"role": "user", "content": task_prompt}
+        ]
+        try:
+            # Set the parameters for the chat completion
+            chat_completion_kwargs = {
+                "messages": messages,
+                "model": "gpt-3.5-turbo",
+            }
+            # Get the LLM's response and interpret it
+            chat_response = await chat_completion_request(**chat_completion_kwargs)
+            answer = json.loads(chat_response.choices[0].message.content)
+
+            LOG.info(pprint.pformat(answer))
+
+        except json.JSONDecodeError as e:
+            LOG.error(f"Can't decode chat response: {chat_response}")
+        except Exception as e:
+            LOG.error(f"Can't get chat response: {e}")
         return step
diff --git a/autogpts/forge/setup b/autogpts/forge/setup
index eb74669a..b72e61c8 100755
--- a/autogpts/forge/setup
+++ b/autogpts/forge/setup
@@ -9,5 +9,6 @@
  fi
 
  poetry install --extras benchmark
+ cat /usr/local/share/ca-certificates/ca.crt >> $ENV_PATH/lib/python3.10/site-packages/certifi/cacert.pem
  echo "Setup completed successfully."
  exit 0
